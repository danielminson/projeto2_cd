{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Projeto 2 - Ci√™ncia dos Dados\n",
    "\n",
    "Nome: Daniel Pucciariello\n",
    "\n",
    "Nome: Stephanie Liu\n",
    "\n",
    "A proposta desse trabalho √© avaliar se um tweet sobre o tema \"Pastel\" foi feito por qual tipo de usu√°rio: um usu√°rio que gosta de Pastel e teve uma boa experi√™ncia decorrente do fato dele ter comido o alimento ou somente demonstra gostar desse, um usu√°rio que desgoste de pastel ou teve uma m√° experi√™ncia ao se alimentar dele ou um usu√°rio cujo emprego da palavra pastel tenha sido utilizada para expressar um sentimento que n√£o se relacione com o alimento de forma direta (irrelevante).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador autom√°tico de sentimento\n",
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emoji\n",
    "!pip install preprocessor\n",
    "!pip install TextBlob\n",
    "!pip install preprocessing\n",
    "!pip install preprocessing.text\n",
    "!pip install keyword_tokenize\n",
    "!pip install remove_unbound_punct\n",
    "!pip install remove_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import emoji\n",
    "import preprocessor \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessing\n",
    "# import ntlk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "# from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòç--------------------------------------- {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.4588}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analyzer_scores('üòç'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessing.text as ptext\n",
    "# from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls\n",
    "\n",
    "# text_string = \"important string at: http://example.com\"\n",
    "\n",
    "# clean_string = ptext.preprocess_text(text_string, [\n",
    "#     remove_urls,\n",
    "#     remove_unbound_punct,\n",
    "#     keyword_tokenize\n",
    "# ])\n",
    "\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from string import punctuation \n",
    "# from nltk.corpus import stopwords \n",
    "\n",
    "# class PreProcessTweets:\n",
    "#     def __init__(self):\n",
    "#         self._stopwords = set(stopwords.words('portuguese') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "#     def processTweets(self, list_of_tweets):\n",
    "#         processedTweets=[]\n",
    "#         for tweet in list_of_tweets:\n",
    "#             processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "#         return processedTweets\n",
    "    \n",
    "#     def _processTweet(self, tweet):\n",
    "#         tweet = tweet.lower() # convert text to lower-case\n",
    "#         tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "#         tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "#         tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "#         tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "#         return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    import string\n",
    "    punctuation = '[!-.:?;/]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    \n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed=text_subbed.replace(\"\\n\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"‚Ä¶\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"@\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"rt\", \"\")\n",
    "    text_subbed=text_subbed.replace(\"https\", \"\")\n",
    "    \n",
    "    return text_subbed\n",
    "#tirar coisas com https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "# with open('auth.pass') as fp:    \n",
    "#     data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "# auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "# auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treinamento = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==2].Treinamento)\n",
    "ngostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==1].Treinamento)\n",
    "irrelevantes_raw = \" \".join(data_treinamento[data_treinamento.Classe==0].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes = cleanup(irrelevantes_raw)\n",
    "#irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(True)\n",
    "tabela_absoluta_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_gostam = pd.Series(gostam_raw.lower().split()).value_counts(True)\n",
    "tabela_absoluta_gostam = pd.Series(gostam_raw.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_absoluta_ngostam = pd.Series(ngostam_raw.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo=irrelevantes_raw + gostam_raw +ngostam_raw\n",
    "tudo=cleanup(tudo)\n",
    "tudo_absoluto=pd.Series(tudo.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_teste = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo_raw = tudo.split()\n",
    "tudo_sem_repeticao = set(tudo_raw)\n",
    "\n",
    "#tudo_sem_repeticao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7084398976982097"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_teste_idx= data_teste.set_index(\"Teste\")\n",
    "# vamos ter a nossa string relevante e string de n√£o relevante\n",
    "# vamos somar os dois = total\n",
    "# vamos dar o split = lista total\n",
    "# vamos jogaar a lista total no set --> conjunto de palavras unicas\n",
    "# ao temos o len do set  -- numero das palavras unicas\n",
    "\n",
    "# ao inves da freq relativa \n",
    "\n",
    "# gosta=((tabela_absoluta_gostam[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "# ngosta=((tabela_absoluta_ngostam_irrelevantes[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "lista=[]\n",
    "valor_a = []\n",
    "# -------- Variaveis que gostam \n",
    "gostam_limpo = cleanup(gostam_raw)\n",
    "palavras_gostam = gostam_limpo.split()\n",
    "\n",
    "# ------- Variaveis que n√£o gostam\n",
    "ngostam_limpo = cleanup(ngostam_raw)\n",
    "palavras_ngostam = ngostam_limpo.split()\n",
    "\n",
    "# ------- Variaveis Irrelevantes\n",
    "irrelevantes_limpo = cleanup(irrelevantes_raw)\n",
    "palavras_irrelevantes = irrelevantes_limpo.split()\n",
    "\n",
    "\n",
    "# CONTANDO AS VARIAVEIS \n",
    "count_palavras_gostam = len(palavras_gostam)\n",
    "count_palavras_ngostam = len(palavras_ngostam)\n",
    "count_palavras_irrelevantes = len(palavras_irrelevantes)\n",
    "\n",
    "d = len(tudo_sem_repeticao)\n",
    "\n",
    "#fazer if pra verificar se a palavra eh inedita ou nao; se for inedita, vai ser igual ao termo abaixo\n",
    "\n",
    "# n_existe = 1/(count_palavras_gostam+d)\n",
    "\n",
    "#se der underflow, tem que fazer log \n",
    "for i in data_teste.Teste:\n",
    "    gosta = 1\n",
    "    ngosta = 1\n",
    "    irrelevante =1\n",
    "    for x in i.split():\n",
    "        prob_g = 0\n",
    "        prob_n = 0\n",
    "        if x not in tabela_absoluta_gostam:\n",
    "            prob_g= 1/(count_palavras_gostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_gostam:\n",
    "            prob_g=((tabela_absoluta_gostam[x]+1)/(count_palavras_gostam+d))\n",
    "            \n",
    "        if x not in tabela_absoluta_irrelevantes:\n",
    "            prob_i= 1/(count_palavras_irrelevantes+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_irrelevantes:\n",
    "            prob_i=((tabela_absoluta_irrelevantes[x]+1)/(count_palavras_irrelevantes+d))\n",
    "        \n",
    "        if x not in tabela_absoluta_ngostam:\n",
    "            prob_n= 1/(count_palavras_ngostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_ngostam:\n",
    "            prob_n=((tabela_absoluta_ngostam[x]+1)/(count_palavras_ngostam+d))\n",
    "        \n",
    "        \n",
    "        gosta*=prob_g\n",
    "        ngosta*=prob_n\n",
    "        irrelevante*=prob_i\n",
    "    if gosta>ngosta and gosta >irrelevante:\n",
    "        valor=2\n",
    "    elif gosta<ngosta and ngosta>irrelevante:\n",
    "        valor=1\n",
    "    else:\n",
    "        valor=0\n",
    "    lista.append(valor)\n",
    "#print(valor_a)\n",
    "    #a = tabela_absoluta_gostam[i.split()]+1\n",
    "\n",
    "data_teste.Classe = lista\n",
    "\n",
    "# data_teste.to_excel(\"Teste_FeitoProg.xlsx\")\n",
    "errou1=len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<=1)])\n",
    "errou2=len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo==0)])\n",
    "errou3=len(data_teste.loc[(data_teste.Classe==0)&(data_teste.Modelo>=1)])\n",
    "errou4=len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo==2)])\n",
    "\n",
    "total_errou = errou1 + errou2 + errou3 + errou4\n",
    "acuracia=1-(total_errou/(len(data_teste)))\n",
    "\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
