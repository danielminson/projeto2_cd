{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Projeto 2 - Ci√™ncia dos Dados\n",
    "\n",
    "Nome: Daniel Pucciariello\n",
    "\n",
    "Nome: Stephanie Liu\n",
    "\n",
    "A proposta desse trabalho √© avaliar se um tweet sobre o tema \"Pastel\" foi feito por qual tipo de usu√°rio: um usu√°rio que gosta de Pastel e teve uma boa experi√™ncia decorrente do fato dele ter comido o alimento ou somente demonstra gostar desse, um usu√°rio que desgoste de pastel ou teve uma m√° experi√™ncia ao se alimentar dele ou um usu√°rio cujo emprego da palavra pastel tenha sido utilizada para expressar um sentimento que n√£o se relacione com o alimento de forma direta (irrelevante).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador autom√°tico de sentimento\n",
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emoji\n",
    "!pip install preprocessor\n",
    "!pip install TextBlob\n",
    "!pip install preprocessing\n",
    "!pip install preprocessing.text\n",
    "!pip install keyword_tokenize\n",
    "!pip install remove_unbound_punct\n",
    "!pip install remove_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import emoji\n",
    "import preprocessor \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessing\n",
    "# from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/anaconda3/lib/python3.7/site-packages/preprocessing/data/bnc_wiktionary_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-9bff4bca09d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeyword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_unbound_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"important string at: http://example.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/preprocessing/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspellcheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/preprocessing/spellcheck.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mEN_ALPHABET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'abcdefghijklmnopqrstuvwxyz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mWORD_DISTRIBUTION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/bnc_wiktionary_corpus.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/anaconda3/lib/python3.7/site-packages/preprocessing/data/bnc_wiktionary_corpus.txt'"
     ]
    }
   ],
   "source": [
    "import preprocessing.text as ptext\n",
    "from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls\n",
    "\n",
    "text_string = \"important string at: http://example.com\"\n",
    "\n",
    "clean_string = ptext.preprocess_text(text_string, [\n",
    "    remove_urls,\n",
    "    remove_unbound_punct,\n",
    "    keyword_tokenize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    import string\n",
    "    punctuation = '[!-.:?;/]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    \n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed=text_subbed.replace(\"\\n\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"‚Ä¶\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"@\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"rt\", \"\")\n",
    "    text_subbed=text_subbed.replace(\"https\", \"\")\n",
    "    \n",
    "    return text_subbed\n",
    "#tirar coisas com https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "# with open('auth.pass') as fp:    \n",
    "#     data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "# auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "# auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treinamento = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes_raw = \" \".join(data_treinamento[data_treinamento.Classe<=1].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes = cleanup(irrelevantes_raw)\n",
    "#irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_ngostam_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(True)\n",
    "tabela_absoluta_ngostam_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==2].Treinamento)\n",
    "ngostam_raw = \" \".join(data_treinamento[data_treinamento.Classe<2].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_gostam = pd.Series(gostam_raw.lower().split()).value_counts(True)\n",
    "tabela_absoluta_gostam = pd.Series(gostam_raw.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiplicando_tabela_naogostam = tabela_relativa_ngostam_irrelevantes*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo=irrelevantes_raw + gostam_raw\n",
    "tudo=cleanup(tudo)\n",
    "tudo_absoluto=pd.Series(tudo.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_teste = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo_raw = tudo.split()\n",
    "tudo_sem_repeticao = set(tudo_raw)\n",
    "\n",
    "#tudo_sem_repeticao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessing' has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-edef40758bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"O daniel √© lalalala üòÄ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'preprocessing' has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "a = preprocessing.text.clean(\"O daniel √© lalalala üòÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessor' has no attribute 'clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-8fc85a986b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Variaveis gostam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgostam_limpo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgostam_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# gostam_limpo = cleanup(gostam_raw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpalavras_gostam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgostam_limpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'preprocessor' has no attribute 'clean'"
     ]
    }
   ],
   "source": [
    "#data_teste_idx= data_teste.set_index(\"Teste\")\n",
    "# vamos ter a nossa string relevante e string de n√£o relevante\n",
    "# vamos somar os dois = total\n",
    "# vamos dar o split = lista total\n",
    "# vamos jogaar a lista total no set --> conjunto de palavras unicas\n",
    "# ao temos o len do set  -- numero das palavras unicas\n",
    "\n",
    "# ao inves da freq relativa \n",
    "\n",
    "#     gosta=((tabela_absoluta_gostam[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "#     ngosta=((tabela_absoluta_ngostam_irrelevantes[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "lista=[]\n",
    "valor_a = []\n",
    "\n",
    "#Variaveis gostam \n",
    "gostam_limpo = p.clean(gostam_raw)\n",
    "# gostam_limpo = cleanup(gostam_raw)\n",
    "palavras_gostam = gostam_limpo.split()\n",
    "ngostam_limpo = cleanup(ngostam_raw)\n",
    "\n",
    "#Variaveis que n√£o gostam\n",
    "palavras_ngostam = ngostam_limpo.split()\n",
    "count_palavras_gostam = len(palavras_gostam)\n",
    "count_palavras_ngostam = len(palavras_ngostam)\n",
    "\n",
    "d = len(tudo_sem_repeticao)\n",
    "\n",
    "#fazer if pra verificar se a palavra eh inedita ou nao; se for inedita, vai ser igual ao termo abaixo\n",
    "\n",
    "n_existe = 1/(count_palavras_gostam+d)\n",
    "\n",
    "#se der underflow, tem que fazer log \n",
    "for i in data_teste.Teste:\n",
    "    gosta = 1\n",
    "    ngosta = 1\n",
    "    for x in i.split():\n",
    "        prob_g = 0\n",
    "        prob_n = 0\n",
    "        if x not in tabela_absoluta_gostam:\n",
    "            prob_g= 1/(count_palavras_gostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_gostam:\n",
    "            prob_g=((tabela_absoluta_gostam[x]+1)/(count_palavras_gostam+d))\n",
    "            \n",
    "        if x not in tabela_absoluta_ngostam_irrelevantes:\n",
    "            prob_n= 1/(count_palavras_ngostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_ngostam_irrelevantes:\n",
    "            prob_n=((tabela_absoluta_ngostam_irrelevantes[x]+1)/(count_palavras_ngostam+d))\n",
    "            \n",
    "        gosta*=prob_g\n",
    "        ngosta*=prob_n\n",
    "    \n",
    "    if gosta>ngosta:\n",
    "        valor=2\n",
    "    elif gosta<ngosta:\n",
    "        valor=1\n",
    "    a = tabela_absoluta_gostam[i.split()]+1\n",
    "    valor_a.append(a)\n",
    "    lista.append(valor)\n",
    "#print(valor_a)\n",
    "\n",
    "data_teste.Classe = lista\n",
    "# data_teste\n",
    "# data_teste.to_excel(\"Teste_FeitoProg.xlsx\")\n",
    "errou=len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<=1)])\n",
    "acuracia=1-(errou/(len(data_teste)))\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
