{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Projeto 2 - Ciência dos Dados\n",
    "\n",
    "Nome: Daniel Pucciariello\n",
    "\n",
    "Nome: Stephanie Liu\n",
    "\n",
    "A proposta desse trabalho é avaliar se um tweet sobre o tema \"Pastel\" foi feito por qual tipo de usuário: um usuário que gosta de Pastel e teve uma boa experiência decorrente do fato dele ter comido o alimento ou somente demonstra gostar desse, um usuário que desgoste de pastel ou teve uma má experiência ao se alimentar dele ou um usuário cujo emprego da palavra pastel tenha sido utilizada para expressar um sentimento que não se relacione com o alimento de forma direta (irrelevante).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n",
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emoji\n",
    "!pip install preprocessor\n",
    "!pip install TextBlob\n",
    "!pip install preprocessing\n",
    "!pip install preprocessing.text\n",
    "!pip install keyword_tokenize\n",
    "!pip install remove_unbound_punct\n",
    "!pip install remove_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import emoji\n",
    "import preprocessor \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessing\n",
    "# from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/anaconda3/lib/python3.7/site-packages/preprocessing/data/bnc_wiktionary_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-9bff4bca09d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeyword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_unbound_punct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"important string at: http://example.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/preprocessing/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspellcheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/preprocessing/spellcheck.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mEN_ALPHABET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'abcdefghijklmnopqrstuvwxyz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mWORD_DISTRIBUTION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/bnc_wiktionary_corpus.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/anaconda3/lib/python3.7/site-packages/preprocessing/data/bnc_wiktionary_corpus.txt'"
     ]
    }
   ],
   "source": [
    "import preprocessing.text as ptext\n",
    "from preprocessing.text import keyword_tokenize, remove_unbound_punct, remove_urls\n",
    "\n",
    "text_string = \"important string at: http://example.com\"\n",
    "\n",
    "clean_string = ptext.preprocess_text(text_string, [\n",
    "    remove_urls,\n",
    "    remove_unbound_punct,\n",
    "    keyword_tokenize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    import string\n",
    "    punctuation = '[!-.:?;/]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    \n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed=text_subbed.replace(\"\\n\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"…\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"@\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"rt\", \"\")\n",
    "    text_subbed=text_subbed.replace(\"https\", \"\")\n",
    "    \n",
    "    return text_subbed\n",
    "#tirar coisas com https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "# with open('auth.pass') as fp:    \n",
    "#     data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "# auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "# auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treinamento = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes_raw = \" \".join(data_treinamento[data_treinamento.Classe<=1].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes = cleanup(irrelevantes_raw)\n",
    "#irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_ngostam_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(True)\n",
    "tabela_absoluta_ngostam_irrelevantes = pd.Series(irrelevantes.lower().split()).value_counts(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==2].Treinamento)\n",
    "ngostam_raw = \" \".join(data_treinamento[data_treinamento.Classe<2].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_gostam = pd.Series(gostam_raw.lower().split()).value_counts(True)\n",
    "tabela_absoluta_gostam = pd.Series(gostam_raw.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiplicando_tabela_naogostam = tabela_relativa_ngostam_irrelevantes*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo=irrelevantes_raw + gostam_raw\n",
    "tudo=cleanup(tudo)\n",
    "tudo_absoluto=pd.Series(tudo.lower().split()).value_counts(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_teste = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo_raw = tudo.split()\n",
    "tudo_sem_repeticao = set(tudo_raw)\n",
    "\n",
    "#tudo_sem_repeticao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessing' has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-edef40758bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"O daniel é lalalala 😀\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'preprocessing' has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "a = preprocessing.text.clean(\"O daniel é lalalala 😀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessor' has no attribute 'clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-8fc85a986b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Variaveis gostam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgostam_limpo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgostam_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# gostam_limpo = cleanup(gostam_raw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpalavras_gostam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgostam_limpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'preprocessor' has no attribute 'clean'"
     ]
    }
   ],
   "source": [
    "#data_teste_idx= data_teste.set_index(\"Teste\")\n",
    "# vamos ter a nossa string relevante e string de não relevante\n",
    "# vamos somar os dois = total\n",
    "# vamos dar o split = lista total\n",
    "# vamos jogaar a lista total no set --> conjunto de palavras unicas\n",
    "# ao temos o len do set  -- numero das palavras unicas\n",
    "\n",
    "# ao inves da freq relativa \n",
    "\n",
    "#     gosta=((tabela_absoluta_gostam[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "#     ngosta=((tabela_absoluta_ngostam_irrelevantes[i.split()]+1)/(count_palavras_gostam+d)).prod()\n",
    "lista=[]\n",
    "valor_a = []\n",
    "\n",
    "#Variaveis gostam \n",
    "gostam_limpo = p.clean(gostam_raw)\n",
    "# gostam_limpo = cleanup(gostam_raw)\n",
    "palavras_gostam = gostam_limpo.split()\n",
    "ngostam_limpo = cleanup(ngostam_raw)\n",
    "\n",
    "#Variaveis que não gostam\n",
    "palavras_ngostam = ngostam_limpo.split()\n",
    "count_palavras_gostam = len(palavras_gostam)\n",
    "count_palavras_ngostam = len(palavras_ngostam)\n",
    "\n",
    "d = len(tudo_sem_repeticao)\n",
    "\n",
    "#fazer if pra verificar se a palavra eh inedita ou nao; se for inedita, vai ser igual ao termo abaixo\n",
    "\n",
    "n_existe = 1/(count_palavras_gostam+d)\n",
    "\n",
    "#se der underflow, tem que fazer log \n",
    "for i in data_teste.Teste:\n",
    "    gosta = 1\n",
    "    ngosta = 1\n",
    "    for x in i.split():\n",
    "        prob_g = 0\n",
    "        prob_n = 0\n",
    "        if x not in tabela_absoluta_gostam:\n",
    "            prob_g= 1/(count_palavras_gostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_gostam:\n",
    "            prob_g=((tabela_absoluta_gostam[x]+1)/(count_palavras_gostam+d))\n",
    "            \n",
    "        if x not in tabela_absoluta_ngostam_irrelevantes:\n",
    "            prob_n= 1/(count_palavras_ngostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_ngostam_irrelevantes:\n",
    "            prob_n=((tabela_absoluta_ngostam_irrelevantes[x]+1)/(count_palavras_ngostam+d))\n",
    "            \n",
    "        gosta*=prob_g\n",
    "        ngosta*=prob_n\n",
    "    \n",
    "    if gosta>ngosta:\n",
    "        valor=2\n",
    "    elif gosta<ngosta:\n",
    "        valor=1\n",
    "    a = tabela_absoluta_gostam[i.split()]+1\n",
    "    valor_a.append(a)\n",
    "    lista.append(valor)\n",
    "#print(valor_a)\n",
    "\n",
    "data_teste.Classe = lista\n",
    "# data_teste\n",
    "# data_teste.to_excel(\"Teste_FeitoProg.xlsx\")\n",
    "errou=len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<=1)])\n",
    "acuracia=1-(errou/(len(data_teste)))\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
