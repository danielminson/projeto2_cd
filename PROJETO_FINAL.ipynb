{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Projeto 2 - Ciência dos Dados\n",
    "\n",
    "Nome: Daniel Pucciariello\n",
    "\n",
    "Nome: Stephanie Liu\n",
    "\n",
    "A proposta desse trabalho é avaliar se um tweet sobre o tema \"Pastel\" foi feito por qual tipo de usuário: um usuário que gosta de Pastel e teve uma boa experiência decorrente do fato dele ter comido o alimento ou somente demonstra gostar desse (classiicação 2), um usuário que desgoste de pastel ou teve uma má experiência ao se alimentar dele (classificação 1) ou um usuário cujo emprego da palavra pastel tenha sido utilizada para expressar um sentimento que não se relacione com o alimento de forma direta (irrelevante foi classificado como 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n",
    "## Preparando o ambiente no jupyter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=red, size=\"5\"> __Importações das bibliotecas__\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "!pip install emoji\n",
    "!pip install remove_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "import string\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=red, size=\"5\"> __Criando as funções para limpeza do código e separação dos emojis__\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_emoji(em):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    \n",
    "    return em_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    punctuation = '[!\\-.:?;/]+' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)    \n",
    "    \n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    text_subbed = re.sub(r\"http\\S+\", \"\", text_subbed)\n",
    "    \n",
    "    text_subbed=text_subbed.replace(\"\\n\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"…\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"@\", \" \")\n",
    "    text_subbed=text_subbed.replace(\"rt\", \"\")\n",
    "    text_subbed=text_subbed.replace(\"..............\", \"\")\n",
    "    text_subbed=text_subbed.replace(\"  \", \" \")\n",
    "    text_subbed=text_subbed.replace(\"   \", \" \")\n",
    "    text_subbed=text_subbed.replace(\"    \", \" \")\n",
    "    text_subbed=text_subbed.replace(\"     \", \" \")\n",
    "    text_subbed=text_subbed.replace(\"      \", \" \")\n",
    "\n",
    "\n",
    "    \n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "# with open('auth.pass') as fp:    \n",
    "#     data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "# auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "# auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treinamento = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==2].Treinamento)\n",
    "ngostam_raw = \" \".join(data_treinamento[data_treinamento.Classe==1].Treinamento)\n",
    "irrelevantes_raw = \" \".join(data_treinamento[data_treinamento.Classe==0].Treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantes = cleanup(irrelevantes_raw)\n",
    "#irrelevantes\n",
    "# -------- Variaveis que gostam \n",
    "gostam_limpo = cleanup(gostam_raw).lower()\n",
    "tabela_absoluta_gostam = pd.Series(split_emoji(gostam_limpo)).value_counts(False)\n",
    "palavras_gostam = split_emoji(gostam_limpo)\n",
    "# ------- Variaveis que não gostam\n",
    "ngostam_limpo = cleanup(ngostam_raw).lower()\n",
    "tabela_absoluta_ngostam = pd.Series(split_emoji(ngostam_limpo)).value_counts(False)\n",
    "palavras_ngostam = split_emoji(ngostam_limpo)\n",
    "\n",
    "# ------- Variaveis Irrelevantes\n",
    "irrelevantes_limpo = cleanup(irrelevantes_raw).lower()\n",
    "tabela_absoluta_irrelevantes = pd.Series(split_emoji(irrelevantes_limpo)).value_counts(False)\n",
    "palavras_irrelevantes = split_emoji(irrelevantes_limpo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo=irrelevantes_raw + gostam_raw +ngostam_raw\n",
    "tudo=cleanup(tudo).lower()\n",
    "tudo_absoluto=pd.Series(split_emoji(tudo)).value_counts(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_teste = pd.read_excel(\"Excel_pastel.xlsx\",sheet_name=\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudo_raw = tudo.split()\n",
    "tudo_sem_repeticao = set(tudo_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Listas Vazias que irão receber valores -----------\n",
    "lista=[]\n",
    "teste_split_emoji=[]\n",
    "\n",
    "# ------- CONTANDO AS VARIAVEIS -----------\n",
    "count_palavras_gostam = len(palavras_gostam) #contagem das palavras classificadas como gosta\n",
    "count_palavras_ngostam = len(palavras_ngostam) #contagem das palavras classificadas como não gosta\n",
    "count_palavras_irrelevantes = len(palavras_irrelevantes) #contagem das palavras irrelevantes\n",
    "d = len(tudo_sem_repeticao) #numero de palavras sem repetição\n",
    "\n",
    "# ------- Naive-Bayes -----------\n",
    "for i in data_teste.Teste:\n",
    "    a=\"\".join(i)\n",
    "    b = split_emoji(cleanup(a).lower())\n",
    "    teste_split_emoji.append(b)\n",
    "    \n",
    "for y in teste_split_emoji:\n",
    "    gosta = 1\n",
    "    ngosta = 1\n",
    "    irrelevante =1\n",
    "    for x in y:\n",
    "        prob_g = 1\n",
    "        prob_n = 1\n",
    "        prob_i = 1\n",
    "        if x not in tabela_absoluta_gostam:\n",
    "            prob_g= 1/(count_palavras_gostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_gostam:\n",
    "            prob_g=((tabela_absoluta_gostam[x]+1)/(count_palavras_gostam+d))\n",
    "            \n",
    "        if x not in tabela_absoluta_irrelevantes:\n",
    "            prob_i= 1/(count_palavras_irrelevantes+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_irrelevantes:\n",
    "            prob_i=((tabela_absoluta_irrelevantes[x]+1)/(count_palavras_irrelevantes+d))\n",
    "        \n",
    "        if x not in tabela_absoluta_ngostam:\n",
    "            prob_n= 1/(count_palavras_ngostam+d)\n",
    "            \n",
    "        elif x in tabela_absoluta_ngostam:\n",
    "            prob_n=((tabela_absoluta_ngostam[x]+1)/(count_palavras_ngostam+d))\n",
    "        \n",
    "        gosta*=prob_g\n",
    "        ngosta*=prob_n\n",
    "        irrelevante*=prob_i\n",
    "        \n",
    "    if gosta>ngosta and gosta >irrelevante:\n",
    "        valor=2\n",
    "    elif gosta<ngosta and ngosta>irrelevante:\n",
    "        valor=1\n",
    "    elif irrelevante>gosta and irrelevante>ngosta:\n",
    "        valor=0\n",
    "    lista.append(valor)\n",
    "\n",
    "data_teste.Classe = lista #coluna do data_frame recebe a lista\n",
    "\n",
    "data_teste.to_excel(\"Planilha_tweets_classificados_por_probabilidade.xlsx\") #criaria o excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7340153452685422"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errou1=len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<=1)])\n",
    "errou2=len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo==0)])\n",
    "errou3=len(data_teste.loc[(data_teste.Classe==0)&(data_teste.Modelo>=1)])\n",
    "errou4=len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo==2)])\n",
    "\n",
    "total_errou = errou1 + errou2 + errou3 + errou4\n",
    "acuracia = 1 - (total_errou/(len(data_teste)))\n",
    "acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_positive1 = len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo==2)])/(len(data_teste))\n",
    "# true_negative1 = len(data_teste.loc[(data_teste.Classe<2)&(data_teste.Modelo<2)])/(len(data_teste))\n",
    "\n",
    "# false_positive1 = len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<2)])/(len(data_teste))\n",
    "# false_negative1 = len(data_teste.loc[(data_teste.Classe<2)&(data_teste.Modelo==2)])/(len(data_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentagem de verdadeiros positivos(mensagens que forem corretamente classificadas como 2):59.846547314578004\n",
      "Porcentagem de verdadeiros negativos(mensagens que foram corretamente classificadas como 1):0.7672634271099744\n",
      "Porcentagem de verdadeiros negativos(mensagens que foram corretamente classificadas como 0):12.787723785166241\n",
      "Porcentagem de falso positivo (mensagens que foram classificadas como diferente de 2):12.787723785166241\n",
      "Porcentagem de falso negativos (mensagens que foram classificadas como diferente de 1):5.626598465473146\n",
      "Porcentagem de falso irrelevante (mensagens que foram classificadas como diferente de 0):8.184143222506394\n"
     ]
    }
   ],
   "source": [
    "true_positive = len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo==2)])/(len(data_teste))*100\n",
    "true_negative = len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo==1)])/(len(data_teste))*100\n",
    "true_irrelevant = len(data_teste.loc[(data_teste.Classe==0)&(data_teste.Modelo==0)])/(len(data_teste))*100\n",
    "\n",
    "false_positive = len(data_teste.loc[(data_teste.Classe==2)&(data_teste.Modelo<2)])/(len(data_teste))*100\n",
    "false_negative = len(data_teste.loc[(data_teste.Classe==1)&(data_teste.Modelo!=1)])/(len(data_teste))*100\n",
    "false_irrelevant = len(data_teste.loc[(data_teste.Classe==0)&(data_teste.Modelo!=0)])/(len(data_teste))*100\n",
    "\n",
    "print(\"Porcentagem de verdadeiros positivos(mensagens que forem corretamente classificadas como 2):{0}\".format(true_positive)) \n",
    "print(\"Porcentagem de verdadeiros negativos(mensagens que foram corretamente classificadas como 1):{0}\".format(true_negative))\n",
    "print(\"Porcentagem de verdadeiros negativos(mensagens que foram corretamente classificadas como 0):{0}\".format(true_irrelevant))\n",
    "\n",
    "print(\"Porcentagem de falso positivo (mensagens que foram classificadas como diferente de 2):{0}\".format(false_positive)) \n",
    "print(\"Porcentagem de falso negativos (mensagens que foram classificadas como diferente de 1):{0}\".format(false_negative))\n",
    "print(\"Porcentagem de falso irrelevante (mensagens que foram classificadas como diferente de 0):{0}\".format(false_irrelevant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo\n",
    "\n",
    "O Projeto tem como base o Teorema de Bayes, mais especificamente Classificador Naive-Bayes. Este assume que os elementos probabilísticos que irão compor um evento, cuja classificação irá ser realizada são independentes entre si. Na prática, isso significa que o cálculo da probabilidade final de acontecimento de determinado evento pode ser simplificada pela multiplicação da probabilidade de cada um dos elementos que o compõe. Aplicações plausíveis e utilizadas frequentemente são os classificadores de SPAM de emails e classificadores de assunto em um conjuntos de notícias, por exemplo. Contudo, neste projeto, usaremos o teorema para classificar tweets.\n",
    "\n",
    "Primeiramente, foram coletados tweets 499 tweets não repetidos de acordo com 3 requisitos, de modo a criar uma base de treinamento após estes terem sua classificação realizada.. A partir disso, foi calculada a probabilidade de cada tweet ser de uma categoria ou de outra. Para tal cálculo foi utilizado uma técnica conhecida como Laplace Smoothing aliada ao conceito de classificador Naive-Bayes e dos dados anteriormente obtidos pela classificação manualmente feita.\n",
    "\n",
    "Devido a grande quantidade de retweets, foi necessário coletar novamente os tweets para serem classificados. Contudo, neste mesmo dia foi o lançamento do novo iPhone, tendo assim comentários imprevistos, em relação a cor pastel dos novos produtos. Isso ocasionou uma interferência nos cálculos da probabilidade, devido a quantidades desproporcionais na base de treinamento e na de teste, levando uma acurácia menor do que prevista. Além disso, é necessário frisar que a quantidade de tweets classificados como negativos é significativamente baixa, o que fez com que o índice de verdadeiros negativos (classificados como 1) fosse menor do que 1%. Apesar dos imprevistos da cor pastel e a baixa quantidade de dados classificados na base de treinamento como 1, a acurácia da classificação chegou a ser maior do que 70%, o que provavelmente se relaciona com a grande aceitação de pastel na sociedade brasileira.\n",
    "\n",
    "Para futuras interações do projeto seria ideal aumentar os tweets da base de treinamento e classificá-las a mão de modo que estes tweets abordassem diferentes assuntos relacionados a pastel, tanto a cor quanto a comida. Além de não ter sido abordado a cor pastel, há outras possíveis influências, como as figuras de linguagem que poderá estar presente no texto, como eufemismo, metáfora, ironia, e entre outros. Para tais situações deve-se implementar um mecanismo que transforma os verbos em seus respectivos infinitivos, de maneira a melhorar a acurácia. Este processo é conhecido como stemming e lemmatization e pode ser empregado utilizando bibliotecas do python como o NTLK (baseado no link da primeira referência).\n",
    "\n",
    "Outro fator importante é que não se deve usar o próprio classificador para gerar mais amostras de treinamento, pois apesar de haver uma comparação de uma probabilidade de ser cada classificação, não há a certeza que tal resultado está certo, logo é inviável concluir que está correto em todos os momentos. Em outras palavras, caso seja utilizado o próprio classificador com a finalidade de gerar mais amostras de treinamento, estaríamos supondo que o nosso classificador está sempre certo, resultando-se em uma acurácia cada vez menor.\n",
    "\n",
    "Por fim, é necessário ressaltar que o próprio Teorema de Bayes é uma grande limitação quando se trata de classificação de praticamente qualquer coisa. No caso de tweets isso não deixa de ser uma exceção, visto que os elementos que compõem as frases (palavras) claramente possuem uma relação entre elas, de maneira que uma irá influenciar na presença das outras palavras e o todo irá influenciar no sentido da frase (nesse caso, um tweet).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
